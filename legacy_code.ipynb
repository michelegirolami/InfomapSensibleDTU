{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_multiplex_network_communities(layers,k_):\n",
    "    \n",
    "    layer12_concat = pd.concat(layers,axis=0)\n",
    "    nodes, edges = network_reformat(layer12_concat, weighted=True, output_format='graph',node_format=\"hash\")\n",
    "    \n",
    "    hash_to_int = dict(zip(nodes,range(len(nodes))))\n",
    "    int_to_hash = dict(zip(range(len(nodes)),nodes))\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([hash_to_int[n] for n in nodes])\n",
    "    G.add_weighted_edges_from([(hash_to_int[i],hash_to_int[j], w) for i,j,w in edges])\n",
    "    #nx.set_node_attributes(G, 'community', node_communities)\n",
    "    \n",
    "    \n",
    "    node_communities = [set([hash_to_int[n] for n in c]) for c in community_detection_multiplex(*layers)]\n",
    "    \n",
    "    \n",
    "    return node_communities\n",
    "    \n",
    "    \n",
    "    \n",
    "    return nx.get_node_attributes(G, 'community')\n",
    "    \n",
    "    # Initialise high-level graph\n",
    "    G_communities = nx.Graph()\n",
    "\n",
    "    community_nodes = list(set(communities))\n",
    "    \n",
    "    # Find edges in the community network (slightly more complicated)\n",
    "    community_edges = {} #Hash table because it makes collapsing edges easier\n",
    "    internal_edges = [0] * num_communities #Counting num of links within each cluster\n",
    "\n",
    "    for (i,j) in G.edges():\n",
    "        \n",
    "        c_i = communities[i]\n",
    "        c_j = communities[j]\n",
    "        \n",
    "        w_ij = G.get_edge_data(i,j)['weight']\n",
    "        \n",
    "        if c_i != c_j:\n",
    "            key = ' '.join([str(num) for num in sorted([c_i,c_j])]) # Example: '1 2'\n",
    "            if key not in community_edges:\n",
    "                community_edges[key] = w_ij\n",
    "            else:\n",
    "                community_edges[key] += w_ij\n",
    "        else:\n",
    "            internal_edges[c_i] += w_ij\n",
    "    \n",
    "    community_edges = [tuple([int(e) for e in k.split(\" \")]+[v]) for (k,v) in community_edges.items()] # Horrible I know..\n",
    "    \n",
    "    G_communities.add_nodes_from(community_nodes)\n",
    "    G_communities.add_weighted_edges_from(community_edges)\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    # BOILERPLATE #\n",
    "    ###############\n",
    "    \n",
    "    pos = nx.spring_layout(G_communities, k=k_, iterations=10)\n",
    "    \n",
    "    # color map from http://colorbrewer2.org/\n",
    "    cmapLight = colors.ListedColormap(['#a6cee3', '#b2df8a', '#fb9a99', '#fdbf6f', '#cab2d6'], 'indexed', num_communities)\n",
    "    cmapDark = colors.ListedColormap(['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a'], 'indexed', num_communities)\n",
    "\n",
    "    # edges\n",
    "    weights_transformed = np.log(np.array([w for (i,j,w) in community_edges]))**3+100\n",
    "    nx.draw_networkx_edges(G_communities, \n",
    "                           pos,\n",
    "                           width=weights_transformed/max(weights_transformed)\n",
    "    )\n",
    "\n",
    "    # nodes\n",
    "    nodeCollection = nx.draw_networkx_nodes(G_communities,\n",
    "        pos = pos,\n",
    "        node_color = G_communities.nodes(),\n",
    "        cmap = cmapLight,\n",
    "        node_size = ((np.array(internal_edges))*1.85/(np.array(internal_edges)).max()+0.15)*1000\n",
    "    )\n",
    "    # set node border color to the darker shade\n",
    "    darkColors = [cmapDark(v) for v in G_communities.nodes()]\n",
    "    nodeCollection.set_edgecolor(darkColors)\n",
    "    \n",
    "    # Node labels\n",
    "    for n in G_communities.nodes_iter():\n",
    "        plt.annotate(n,\n",
    "            xy = pos[n],\n",
    "            textcoords = 'offset points',\n",
    "            horizontalalignment = 'center',\n",
    "            verticalalignment = 'center',\n",
    "            xytext = [0, 2],\n",
    "            color = cmapDark(n)\n",
    "        )\n",
    "\n",
    "    plt.axis('off')\n",
    "    #plt.savefig(\"netw.png\")\n",
    "    \n",
    "    \n",
    "#draw_multiplex_network_communities([layer1,layer2],k_=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network_reformat_multiplex(halflife=-1,node_representation=\"physical\",*args):\n",
    "    \"\"\"Return multiplex representation of multiplex network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    halflife : number\n",
    "        Halflife in seconds of relax-rate decay between layers.\n",
    "        Defaults to -1.\n",
    "    node_representation : string\n",
    "        Set to \"state if state nodes associated with the same physical\n",
    "        node should have different reference names. Defaults to \"physical\".\n",
    "    *args : pandas df formatted layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    net_file : string\n",
    "        A network string in multiplex format\n",
    "    int_to_hash : dict\n",
    "        Key-value pairs of node integer id and original hash id\n",
    "    \"\"\"\n",
    "    print \"-> network_reformat_multiplex\\n\\t\",\n",
    "    \n",
    "    # Infomap will only work with node indices on the command line.\n",
    "    \n",
    "    # Pool all nodes and sort the set\n",
    "    nodes_hash = []\n",
    "    for l, df in enumerate(args):\n",
    "        layer_nodes = set()\n",
    "        layer_nodes.update(df[\"user1\"])\n",
    "        layer_nodes.update(df[\"user2\"])\n",
    "\n",
    "        if node_representation == \"state\":\n",
    "            # Add layer identifier to end of each node hash\n",
    "            layer_nodes = [hash_str+\"_\"+str(l+1) for hash_str in layer_nodes]    \n",
    "        nodes_hash.extend(layer_nodes) # no good reason to sort them\n",
    "        \n",
    "    if node_representation == \"physical\":\n",
    "        nodes_hash = sorted(set(nodes_hash)) # no good reason not to sort them\n",
    "    \n",
    "    nodes = nodes_hash\n",
    "    \n",
    "    # Add vertices\n",
    "    out_file = \"*Vertices %d\" % len(nodes)\n",
    "    \n",
    "    hashid_to_intid = {}\n",
    "    for i,n in enumerate(nodes):\n",
    "        intid = i+1\n",
    "        hashid = str(n)\n",
    "        out_file += '\\n%d \"Node %s\" 1.0' % (intid,hashid)\n",
    "        hashid_to_intid[hashid] = intid\n",
    "        \n",
    "    # Add edges header to file\n",
    "    out_file += \"\\n*Multiplex\"\n",
    "    \n",
    "    for l, df in enumerate(args):\n",
    "        if node_representation == \"state\":\n",
    "            user1 = [hash_str+\"_\"+str(l+1) for hash_str in df[\"user1\"]]\n",
    "            user2 = [hash_str+\"_\"+str(l+1) for hash_str in df[\"user2\"]]\n",
    "        else:\n",
    "            user1 = df[\"user1\"]\n",
    "            user2 = df[\"user2\"]\n",
    "            \n",
    "        edges_hash = zip(user1, user2)\n",
    "        \n",
    "        # Add weights. THIS IS REDUNDANT FOR 5MINS TIMESLICES BECAUSE PPL ONLY MEET ONCE HERE.\n",
    "        edges = [(e[0],e[1],w) for e,w in Counter(edges_hash).items()]\n",
    "        \n",
    "        # Add edges to file\n",
    "        for i,j,w in edges:\n",
    "            out_file += '\\n%d %s %d %s %d' % (l+1,hashid_to_intid[i],l+1,hashid_to_intid[j],w) #+1 because 1 is first layer index\n",
    "            \n",
    "    # Add interlayer relax-rates - if halflife is not infinte (represented as -1)\n",
    "    if halflife == -1:\n",
    "        return out_file\n",
    "    \n",
    "    # Relax decay function\n",
    "    def N(t):\n",
    "        tau = halflife/np.log(2)\n",
    "        return np.exp(-t/float(tau))\n",
    "    \n",
    "    # Add inter-statenode relax rates\n",
    "    if node_representation == \"state\":\n",
    "        nodes_hash_set = set([hash_str_id.split(\"_\")[0] for hash_str_id in nodes_hash])\n",
    "        error_count = 0\n",
    "        for n in nodes_hash_set:\n",
    "            for l1, df1 in enumerate(args):\n",
    "                for l2, df2 in enumerate(args):\n",
    "                    if not l2 > l1:\n",
    "                        continue\n",
    "                        \n",
    "                    in_both_layers = set(list(df1['user1'].values)+list(df1['user2'].values)) & set(list(df2['user1'].values)+list(df2['user2'].values))\n",
    "                    if n not in in_both_layers:\n",
    "                        continue\n",
    "                        \n",
    "                    n1 = n+\"_\"+str(l1+1)\n",
    "                    n2 = n+\"_\"+str(l2+1)\n",
    "                    \n",
    "                    time_diff = df2['timestamp'].values[0] - df1['timestamp'].values[0]\n",
    "                    out_file += '\\n%d %s %d %s %f' % (l1+1,hashid_to_intid[n1],l2+1,hashid_to_intid[n2],N(time_diff))\n",
    "                    \n",
    "                        \n",
    "    elif node_representation == \"physical\":\n",
    "        for n in nodes:\n",
    "            for l1, df1 in enumerate(args):\n",
    "                for l2, df2 in enumerate(args):\n",
    "                    if not l2 > l1:\n",
    "                        continue\n",
    "                    time_diff = df2['timestamp'].values[0] - df1['timestamp'].values[0]\n",
    "                    out_file += '\\n%d %s %d %s %f' % (l1+1,hashid_to_intid[n],l2+1,hashid_to_intid[n],N(time_diff))\n",
    "    \n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named graph_tool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d4681d01b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraph_tool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named graph_tool"
     ]
    }
   ],
   "source": [
    "from graphtool import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
